{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj4PGgkzyIXg/Y1FLcckQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramyrahmeni/DeepLearning/blob/main/RNN_Vanilla_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PHAm684H7ONT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a Vanilla RNN with Numpy"
      ],
      "metadata": {
        "id": "GyfgxTnj7PE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "inputs = np.array([\n",
        "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
        "    [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
        "    [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
        "    [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
        "    [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
        "])\n",
        "\n",
        "expected = np.array([\n",
        "    [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
        "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
        "    [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"],\n",
        "    [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
        "    [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
        "])\n"
      ],
      "metadata": {
        "id": "-YOCO4c57SbD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def string_to_one_hot(inputs: np.ndarray) -> np.ndarray:\n",
        "    char_to_index = {char: i for i, char in enumerate(string.ascii_uppercase)}\n",
        "\n",
        "    one_hot_inputs = []\n",
        "    for row in inputs:\n",
        "        one_hot_list = []\n",
        "        for char in row:\n",
        "            if char.upper() in char_to_index:\n",
        "                one_hot_vector = np.zeros((len(string.ascii_uppercase), 1))\n",
        "                one_hot_vector[char_to_index[char.upper()]] = 1\n",
        "                one_hot_list.append(one_hot_vector)\n",
        "        one_hot_inputs.append(one_hot_list)\n",
        "\n",
        "    return np.array(one_hot_inputs)"
      ],
      "metadata": {
        "id": "8HPSZWk97eEj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputLayer:\n",
        "    inputs: np.ndarray\n",
        "    U: np.ndarray = None\n",
        "    delta_U: np.ndarray = None\n",
        "\n",
        "    def __init__(self, inputs: np.ndarray, hidden_size: int) -> None:\n",
        "        self.inputs = inputs\n",
        "        self.U = np.random.uniform(low=0, high=1, size=(hidden_size, len(inputs[0])))\n",
        "        self.delta_U = np.zeros_like(self.U)\n",
        "\n",
        "    def get_input(self, time_step: int) -> np.ndarray:\n",
        "        return self.inputs[time_step]\n",
        "\n",
        "    def weighted_sum(self, time_step: int) -> np.ndarray:\n",
        "        return self.U @ self.get_input(time_step)\n",
        "\n",
        "    def calculate_deltas_per_step(\n",
        "        self, time_step: int, delta_weighted_sum: np.ndarray\n",
        "    ) -> None:\n",
        "        # (h_dimension, 1) @ (1, input_size) = (h_dimension, input_size)\n",
        "        self.delta_U += delta_weighted_sum @ self.get_input(time_step).T\n",
        "\n",
        "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
        "        self.U -= learning_rate * self.delta_U"
      ],
      "metadata": {
        "id": "H03kHQF17lvB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HiddenLayer:\n",
        "    states: np.ndarray = None\n",
        "    W: np.ndarray = None\n",
        "    delta_W: np.ndarray = None\n",
        "    bias: np.ndarray = None\n",
        "    delta_bias: np.ndarray = None\n",
        "    next_delta_activation: np.ndarray = None\n",
        "\n",
        "    def __init__(self, vocab_size: int, size: int) -> None:\n",
        "        self.W = np.random.uniform(low=0, high=1, size=(size, size))\n",
        "        self.bias = np.random.uniform(low=0, high=1, size=(size, 1))\n",
        "        self.states = np.zeros(shape=(vocab_size, size, 1))\n",
        "        self.next_delta_activation = np.zeros(shape=(size, 1))\n",
        "        self.delta_bias = np.zeros_like(self.bias)\n",
        "        self.delta_W = np.zeros_like(self.W)\n",
        "\n",
        "    def get_hidden_state(self, time_step: int) -> np.ndarray:\n",
        "        # If starting out at the beginning of the sequence, a[t-1] will return zeros\n",
        "        if time_step < 0:\n",
        "            return np.zeros_like(self.states[0])\n",
        "        return self.states[time_step]\n",
        "\n",
        "    def set_hidden_state(self, time_step: int, hidden_state: np.ndarray) -> None:\n",
        "        self.states[time_step] = hidden_state\n",
        "\n",
        "    def activate(self, weighted_input: np.ndarray, time_step: int) -> np.ndarray:\n",
        "        previous_hidden_state = self.get_hidden_state(time_step - 1)\n",
        "        # W @ h_prev => (h_dimension, h_dimension) @ (h_dimension, 1) = (h_dimension, 1)\n",
        "        weighted_hidden_state = self.W @ previous_hidden_state\n",
        "        # (h_dimension, 1) + (h_dimension, 1) + (h_dimension, 1) = (h_dimension, 1)\n",
        "        weighted_sum = weighted_input + weighted_hidden_state + self.bias\n",
        "        activation = np.tanh(weighted_sum)  # (h_dimension, 1)\n",
        "        self.set_hidden_state(time_step, activation)\n",
        "        return activation\n",
        "\n",
        "    def calculate_deltas_per_step(\n",
        "        self, time_step: int, delta_output: np.ndarray\n",
        "    ) -> np.ndarray:\n",
        "        # (h_dimension, 1) + (h_dimension, 1) = (h_dimension, 1)\n",
        "        delta_activation = delta_output + self.next_delta_activation\n",
        "        # (h_dimension, 1) * scalar = (h_dimension, 1)\n",
        "        delta_weighted_sum = delta_activation * (\n",
        "            1 - self.get_hidden_state(time_step) ** 2\n",
        "        )\n",
        "        # (h_dimension, h_dimension) @ (h_dimension, 1) = (h_dimension, 1)\n",
        "        self.next_delta_activation = self.W.T @ delta_weighted_sum\n",
        "\n",
        "        # (h_dimension, 1) @ (1, h_dimension) = (h_dimension, h_dimension)\n",
        "        self.delta_W += delta_weighted_sum @ self.get_hidden_state(time_step - 1).T\n",
        "\n",
        "        # derivative of hidden bias is the same as dL_ds\n",
        "        self.delta_bias += delta_weighted_sum\n",
        "        return delta_weighted_sum\n",
        "\n",
        "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
        "        self.W -= learning_rate * self.delta_W\n",
        "        self.bias -= learning_rate * self.delta_bias"
      ],
      "metadata": {
        "id": "gr6eggKC73P_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute the softmax of vector x in a numerically stable way.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
        "    return e_x / e_x.sum(axis=0)  # Normalize to get probabilities\n",
        "\n",
        "class OutputLayer:\n",
        "    states: np.ndarray = None\n",
        "    V: np.ndarray = None\n",
        "    bias: np.ndarray = None\n",
        "    delta_bias: np.ndarray = None\n",
        "    delta_V: np.ndarray = None\n",
        "\n",
        "    def __init__(self, size: int, hidden_size: int) -> None:\n",
        "        self.V = np.random.uniform(low=0, high=1, size=(size, hidden_size))\n",
        "        self.bias = np.random.uniform(low=0, high=1, size=(size, 1))\n",
        "        self.states = np.zeros(shape=(size, size, 1))\n",
        "        self.delta_bias = np.zeros_like(self.bias)\n",
        "        self.delta_V = np.zeros_like(self.V)\n",
        "\n",
        "    def predict(self, hidden_state: np.ndarray, time_step: int) -> np.ndarray:\n",
        "        # V @ h => (input_size, h_dimension) @ (h_dimension, 1) = (input_size, 1)\n",
        "        # (input_size, 1) + (input_size, 1) = (input_size, 1)\n",
        "        output = self.V @ hidden_state + self.bias\n",
        "        prediction = softmax(output)\n",
        "        self.set_state(time_step, prediction)\n",
        "        return prediction\n",
        "\n",
        "    def get_state(self, time_step: int) -> np.ndarray:\n",
        "        return self.states[time_step]\n",
        "\n",
        "    def set_state(self, time_step: int, prediction: np.ndarray) -> None:\n",
        "        self.states[time_step] = prediction\n",
        "\n",
        "    def calculate_deltas_per_step(\n",
        "        self,\n",
        "        expected: np.ndarray,\n",
        "        hidden_state: np.ndarray,\n",
        "        time_step: int,\n",
        "    ) -> np.ndarray:\n",
        "        # dL_do = dL_dyhat * dyhat_do = derivative of loss function * derivative of softmax\n",
        "        # dL_do = step.y_hat - expected[step_number]\n",
        "        delta_output = self.get_state(time_step) - expected  # (input_size, 1)\n",
        "\n",
        "        # (input_size, 1) @ (1, hidden_size) = (input_size, hidden_size)\n",
        "        self.delta_V += delta_output @ hidden_state.T\n",
        "\n",
        "        # dL_dc += dL_do\n",
        "        self.delta_bias += delta_output\n",
        "        return self.V.T @ delta_output\n",
        "\n",
        "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
        "        self.V -= learning_rate * self.delta_V\n",
        "        self.bias -= learning_rate * self.delta_bias"
      ],
      "metadata": {
        "id": "oMvCmTli_KJ5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN:\n",
        "    hidden_layer: HiddenLayer\n",
        "    output_layer: OutputLayer\n",
        "    alpha: float  # learning rate\n",
        "    input_layer: InputLayer = None\n",
        "\n",
        "    def __init__(self, vocab_size: int, hidden_size: int, alpha: float) -> None:\n",
        "        self.hidden_layer = HiddenLayer(vocab_size, hidden_size)\n",
        "        self.output_layer = OutputLayer(vocab_size, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def feed_forward(self, inputs: np.ndarray) -> OutputLayer:\n",
        "        self.input_layer = InputLayer(inputs, self.hidden_size)\n",
        "        for step in range(len(inputs)):\n",
        "            weighted_input = self.input_layer.weighted_sum(step)\n",
        "            activation = self.hidden_layer.activate(weighted_input, step)\n",
        "            self.output_layer.predict(activation, step)\n",
        "        return self.output_layer\n",
        "\n",
        "    def backpropagation(self, expected: np.ndarray) -> None:\n",
        "        for step_number in reversed(range(len(expected))):\n",
        "            delta_output = self.output_layer.calculate_deltas_per_step(\n",
        "                expected[step_number],\n",
        "                self.hidden_layer.get_hidden_state(step_number),\n",
        "                step_number,\n",
        "            )\n",
        "            delta_weighted_sum = self.hidden_layer.calculate_deltas_per_step(\n",
        "                step_number, delta_output\n",
        "            )\n",
        "            self.input_layer.calculate_deltas_per_step(step_number, delta_weighted_sum)\n",
        "\n",
        "        self.output_layer.update_weights_and_bias(self.alpha)\n",
        "        self.hidden_layer.update_weights_and_bias(self.alpha)\n",
        "        self.input_layer.update_weights_and_bias(self.alpha)\n",
        "\n",
        "    def loss(self, y_hat: list[np.ndarray], y: list[np.ndarray]) -> float:\n",
        "        \"\"\"\n",
        "        Cross-entropy loss function - Calculating difference between 2 probability distributions.\n",
        "        First, calculate cross-entropy loss for each time step with np.sum, which returns a numpy array\n",
        "        Then, sum across individual losses of all time steps with sum() to get a scalar value.\n",
        "        :param y_hat: predicted value\n",
        "        :param y: expected value - true label\n",
        "        :return: total loss\n",
        "        \"\"\"\n",
        "        return sum(-np.sum(y[i] * np.log(y_hat[i]) for i in range(len(y))))\n",
        "\n",
        "    def train(self, inputs: np.ndarray, expected: np.ndarray, epochs: int) -> None:\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"epoch={epoch}\")\n",
        "            for idx, input in enumerate(inputs):\n",
        "                y_hats = self.feed_forward(input)\n",
        "                self.backpropagation(expected[idx])\n",
        "                print(\n",
        "                    f\"Loss round: {self.loss([y for y in y_hats.states], expected[idx])}\"\n",
        "                )"
      ],
      "metadata": {
        "id": "4FPFyjnT_Rq_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = VanillaRNN(vocab_size=26, hidden_size=128, alpha=0.0001)\n",
        "inputs = np.array([\n",
        "      [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
        "      [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
        "      [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
        "      [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
        "      [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
        "  ])\n",
        "\n",
        "expected = np.array([\n",
        "    [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
        "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
        "    [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"],\n",
        "    [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
        "    [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
        "])\n",
        "\n",
        "one_hot_inputs = string_to_one_hot(inputs)\n",
        "one_hot_expected = string_to_one_hot(expected)\n",
        "\n",
        "# Forward pass through time, no gradient clipping yet so there will be gradient exploding problem\n",
        "# https://stackoverflow.com/a/33980220\n",
        "# https://stackoverflow.com/a/72494516\n",
        "rnn = VanillaRNN(vocab_size=len(string.ascii_uppercase), hidden_size=128, alpha=0.0001)\n",
        "rnn.train(one_hot_inputs, one_hot_expected, epochs=10)\n",
        "\n",
        "new_inputs = np.array([[\"B\", \"C\", \"D\"]])\n",
        "for input in string_to_one_hot(new_inputs):\n",
        "    predictions = rnn.feed_forward(input)\n",
        "    output = np.argmax(predictions.states[-1])\n",
        "    print(output) # index of the one-hot value of prediction\n",
        "    print(string.ascii_uppercase[output]) # mapping one hot to character"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AV_CaSs_kON",
        "outputId": "3abe12ff-b85d-49e4-c5dc-420d2c756741"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0\n",
            "Loss round: [234.97531452]\n",
            "Loss round: [227.06551787]\n",
            "Loss round: [219.76793236]\n",
            "Loss round: [208.49136592]\n",
            "Loss round: [194.16174044]\n",
            "epoch=1\n",
            "Loss round: [180.05266562]\n",
            "Loss round: [165.13615263]\n",
            "Loss round: [155.10746639]\n",
            "Loss round: [148.79333545]\n",
            "Loss round: [140.14736545]\n",
            "epoch=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-971f78e1216c>:46: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  return sum(-np.sum(y[i] * np.log(y_hat[i]) for i in range(len(y))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss round: [134.23998087]\n",
            "Loss round: [126.56765964]\n",
            "Loss round: [124.164305]\n",
            "Loss round: [121.99755419]\n",
            "Loss round: [118.87136242]\n",
            "epoch=3\n",
            "Loss round: [118.50405164]\n",
            "Loss round: [116.43549936]\n",
            "Loss round: [115.70266394]\n",
            "Loss round: [115.40034377]\n",
            "Loss round: [116.58641296]\n",
            "epoch=4\n",
            "Loss round: [118.02569216]\n",
            "Loss round: [117.29331431]\n",
            "Loss round: [119.44059032]\n",
            "Loss round: [120.28396348]\n",
            "Loss round: [122.45632485]\n",
            "epoch=5\n",
            "Loss round: [125.82958788]\n",
            "Loss round: [127.2683031]\n",
            "Loss round: [130.11191893]\n",
            "Loss round: [132.48095862]\n",
            "Loss round: [136.79701142]\n",
            "epoch=6\n",
            "Loss round: [142.20793068]\n",
            "Loss round: [144.75061419]\n",
            "Loss round: [149.32563255]\n",
            "Loss round: [152.38975528]\n",
            "Loss round: [158.1027754]\n",
            "epoch=7\n",
            "Loss round: [164.31477808]\n",
            "Loss round: [168.31969746]\n",
            "Loss round: [172.39072404]\n",
            "Loss round: [173.66824496]\n",
            "Loss round: [181.42794341]\n",
            "epoch=8\n",
            "Loss round: [186.57498359]\n",
            "Loss round: [188.50933205]\n",
            "Loss round: [192.00034603]\n",
            "Loss round: [191.74338414]\n",
            "Loss round: [198.31367827]\n",
            "epoch=9\n",
            "Loss round: [203.17999858]\n",
            "Loss round: [203.1721638]\n",
            "Loss round: [203.91311715]\n",
            "Loss round: [201.38448653]\n",
            "Loss round: [203.43413032]\n",
            "23\n",
            "X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cBCdYjRN7Uax"
      }
    }
  ]
}